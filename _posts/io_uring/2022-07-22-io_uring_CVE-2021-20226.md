---
title: io_uring, CVE-2021-20226
date: 2022-04-01 10:07:00 +09:00
categories: [linux kernel, io_uring]
tags: [io_uring]
description: studying abt io_uring
---

# io_uring 예제

분석할 만한 io_uring 예제가 필요하여 간단한 예제로 먼저 io_uring 동작방식을 kgdb/printk등으로 디버깅

```c
#include <stdio.h>
#include <stdlib.h>
#include <sys/stat.h>
#include <sys/ioctl.h>
#include <sys/syscall.h>
#include <sys/mman.h>
#include <sys/uio.h>
#include <linux/fs.h>
#include <fcntl.h>
#include <unistd.h>
#include <string.h>
#include <linux/io_uring.h>

/* If your compilation fails because the header file below is missing,
 * your kernel is probably too old to support io_uring.
 * */
#include <linux/io_uring.h>

#define QUEUE_DEPTH 1
#define BLOCK_SZ    1024

/* This is x86 specific */
#define read_barrier()  __asm__ __volatile__("":::"memory")
#define write_barrier() __asm__ __volatile__("":::"memory")

struct app_io_sq_ring {
    unsigned *head;
    unsigned *tail;
    unsigned *ring_mask;
    unsigned *ring_entries;
    unsigned *flags;
    unsigned *array;
};

struct app_io_cq_ring {
    unsigned *head;
    unsigned *tail;
    unsigned *ring_mask;
    unsigned *ring_entries;
    struct io_uring_cqe *cqes;
};

struct  submitter {
    int ring_fd;
    struct app_io_sq_ring sq_ring;
    struct io_uring_sqe *sqes;
    struct app_io_cq_ring cq_ring;
};

struct file_info {
    off_t file_sz;
    struct iovec iovecs[];      /* Referred by readv/writev */
};

/*
 * This code is written in the days when io_uring-related system calls are not
 * part of standard C libraries. So, we roll our own system call wrapper
 * functions.
 * */

int io_uring_setup(unsigned entries, struct io_uring_params *p)
{
    return (int) syscall(__NR_io_uring_setup, entries, p);
}

int io_uring_enter(int ring_fd, unsigned int to_submit,
                          unsigned int min_complete, unsigned int flags)
{
    return (int) syscall(__NR_io_uring_enter, ring_fd, to_submit, min_complete,
                   flags, NULL, 0);
}

/*
 * Returns the size of the file whose open file descriptor is passed in.
 * Properly handles regular file and block devices as well. Pretty.
 * */

off_t get_file_size(int fd) {
    struct stat st;

    if(fstat(fd, &st) < 0) {
        perror("fstat");
        return -1;
    }
    if (S_ISBLK(st.st_mode)) {
        unsigned long long bytes;
        if (ioctl(fd, BLKGETSIZE64, &bytes) != 0) {
            perror("ioctl");
            return -1;
        }
        return bytes;
    } else if (S_ISREG(st.st_mode))
        return st.st_size;

    return -1;
}

/*
 * io_uring requires a lot of setup which looks pretty hairy, but isn't all
 * that difficult to understand. Because of all this boilerplate code,
 * io_uring's author has created liburing, which is relatively easy to use.
 * However, you should take your time and understand this code. It is always
 * good to know how it all works underneath. Apart from bragging rights,
 * it does offer you a certain strange geeky peace.
 * */

int app_setup_uring(struct submitter *s) {
    struct app_io_sq_ring *sring = &s->sq_ring;
    struct app_io_cq_ring *cring = &s->cq_ring;
    struct io_uring_params p;
    void *sq_ptr, *cq_ptr;

    /*
     * We need to pass in the io_uring_params structure to the io_uring_setup()
     * call zeroed out. We could set any flags if we need to, but for this
     * example, we don't.
     * */
    memset(&p, 0, sizeof(p));
    s->ring_fd = io_uring_setup(QUEUE_DEPTH, &p);
    if (s->ring_fd < 0) {
        perror("io_uring_setup");
        return 1;
    }

    printf("sq_off.array = %d\n", p.sq_off.array);
    printf("sq_off.head = %d\n", p.sq_off.head);
    /*
     * io_uring communication happens via 2 shared kernel-user space ring buffers,
     * which can be jointly mapped with a single mmap() call in recent kernels. 
     * While the completion queue is directly manipulated, the submission queue 
     * has an indirection array in between. We map that in as well.
     * */

    int sring_sz = p.sq_off.array + p.sq_entries * sizeof(unsigned);
    int cring_sz = p.cq_off.cqes + p.cq_entries * sizeof(struct io_uring_cqe);

    /* In kernel version 5.4 and above, it is possible to map the submission and 
     * completion buffers with a single mmap() call. Rather than check for kernel 
     * versions, the recommended way is to just check the features field of the 
     * io_uring_params structure, which is a bit mask. If the 
     * IORING_FEAT_SINGLE_MMAP is set, then we can do away with the second mmap()
     * call to map the completion ring.
     * */

    /* if (p.flags & IORING_FEAT_SINGLE_MMAP) { */
    /*   if (cring_sz > sring_sz) { */
    /*     sring_sz = cring_sz; */
    /*   } */
    /*   cring_sz = sring_sz; */
    /* } */

    /* Map in the submission and completion queue ring buffers.
     * Older kernels only map in the submission queue, though.
     * */
    sq_ptr = mmap(0, sring_sz, PROT_READ | PROT_WRITE, 
            MAP_SHARED | MAP_POPULATE,
            s->ring_fd, IORING_OFF_SQ_RING);
    if (sq_ptr == MAP_FAILED) {
        perror("mmap");
        return 1;
    }

    /* if (p.flags & IORING_FEAT_SINGLE_MMAP) { */
    /*   cq_ptr = sq_ptr; */
    /* } else { */
      /* Map in the completion queue ring buffer in older kernels separately */
      cq_ptr = mmap(0, cring_sz, PROT_READ | PROT_WRITE,
                    MAP_SHARED | MAP_POPULATE, s->ring_fd, IORING_OFF_CQ_RING);
      if (cq_ptr == MAP_FAILED) {
        perror("mmap");
        return 1;
      }
    /* } */
    /* Save useful fields in a global app_io_sq_ring struct for later
     * easy reference */
    sring->head = sq_ptr + p.sq_off.head;
    sring->tail = sq_ptr + p.sq_off.tail;
    sring->ring_mask = sq_ptr + p.sq_off.ring_mask;
    sring->ring_entries = sq_ptr + p.sq_off.ring_entries;
    sring->flags = sq_ptr + p.sq_off.flags;
    sring->array = sq_ptr + p.sq_off.array;

    /* Map in the submission queue entries array */
    s->sqes = mmap(0, p.sq_entries * sizeof(struct io_uring_sqe),
            PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE,
            s->ring_fd, IORING_OFF_SQES);
    if (s->sqes == MAP_FAILED) {
        perror("mmap");
        return 1;
    }

    /* Save useful fields in a global app_io_cq_ring struct for later
     * easy reference */
    cring->head = cq_ptr + p.cq_off.head;
    cring->tail = cq_ptr + p.cq_off.tail;
    cring->ring_mask = cq_ptr + p.cq_off.ring_mask;
    cring->ring_entries = cq_ptr + p.cq_off.ring_entries;
    cring->cqes = cq_ptr + p.cq_off.cqes;

    return 0;
}

/*
 * Output a string of characters of len length to stdout.
 * We use buffered output here to be efficient,
 * since we need to output character-by-character.
 * */
void output_to_console(char *buf, int len) {
    while (len--) {
        fputc(*buf++, stdout);
    }
}

/*
 * Read from completion queue.
 * In this function, we read completion events from the completion queue, get
 * the data buffer that will have the file data and print it to the console.
 * */

void read_from_cq(struct submitter *s) {
    struct file_info *fi;
    struct app_io_cq_ring *cring = &s->cq_ring;
    struct io_uring_cqe *cqe;
    unsigned head, reaped = 0;

    head = *cring->head;

    do {
        read_barrier();
        /*
         * Remember, this is a ring buffer. If head == tail, it means that the
         * buffer is empty.
         * */
        if (head == *cring->tail)
            break;

        /* Get the entry */
        cqe = &cring->cqes[head & *s->cq_ring.ring_mask];
        fi = (struct file_info*) cqe->user_data;
        if (cqe->res < 0)
            fprintf(stderr, "Error: %s\n", strerror(abs(cqe->res)));

        int blocks = (int) fi->file_sz / BLOCK_SZ;
        if (fi->file_sz % BLOCK_SZ) blocks++;

        for (int i = 0; i < blocks; i++)
            output_to_console(fi->iovecs[i].iov_base, fi->iovecs[i].iov_len);

        head++;
    } while (1);

    *cring->head = head;
    write_barrier();
}
/*
 * Submit to submission queue.
 * In this function, we submit requests to the submission queue. You can submit
 * many types of requests. Ours is going to be the readv() request, which we
 * specify via IORING_OP_READV.
 *
 * */
int submit_to_sq(char *file_path, struct submitter *s) {
    struct file_info *fi;

    int file_fd = open(file_path, O_RDONLY);
    if (file_fd < 0 ) {
        perror("open");
        return 1;
    }

    struct app_io_sq_ring *sring = &s->sq_ring;
    unsigned index = 0, current_block = 0, tail = 0, next_tail = 0;

    off_t file_sz = get_file_size(file_fd);
    if (file_sz < 0)
        return 1;
    off_t bytes_remaining = file_sz;
    int blocks = (int) file_sz / BLOCK_SZ;
    if (file_sz % BLOCK_SZ) blocks++;

    fi = malloc(sizeof(*fi) + sizeof(struct iovec) * blocks);
    if (!fi) {
        fprintf(stderr, "Unable to allocate memory\n");
        return 1;
    }
    fi->file_sz = file_sz;

    /*
     * For each block of the file we need to read, we allocate an iovec struct
     * which is indexed into the iovecs array. This array is passed in as part
     * of the submission. If you don't understand this, then you need to look
     * up how the readv() and writev() system calls work.
     * */
    while (bytes_remaining) {
        off_t bytes_to_read = bytes_remaining;
        if (bytes_to_read > BLOCK_SZ)
            bytes_to_read = BLOCK_SZ;

        fi->iovecs[current_block].iov_len = bytes_to_read;

        void *buf;
        if( posix_memalign(&buf, BLOCK_SZ, BLOCK_SZ)) {
            perror("posix_memalign");
            return 1;
        }
        fi->iovecs[current_block].iov_base = buf;

        current_block++;
        bytes_remaining -= bytes_to_read;
    }

    /* Add our submission queue entry to the tail of the SQE ring buffer */
    next_tail = tail = *sring->tail;
    printf("sring->tail is 0x%x\n", sring->tail);
    printf("next_tail is %d\n", next_tail);
    next_tail++;
    read_barrier();
    index = tail & *s->sq_ring.ring_mask;
    printf("index is %d\n", index);
    struct io_uring_sqe *sqe = &s->sqes[index];
    sqe->fd = file_fd;
    sqe->flags = 0;
    sqe->opcode = IORING_OP_READV;
    sqe->addr = (unsigned long) fi->iovecs;
    sqe->len = blocks;
    sqe->off = 0;
    sqe->user_data = (unsigned long long) fi;
    sring->array[index] = index;
    tail = next_tail;

    /* Update the tail so the kernel can see it. */
    if(*sring->tail != tail) {
        *sring->tail = tail;
        write_barrier();
    }

    /*
     * Tell the kernel we have submitted events with the io_uring_enter() system
     * call. We also pass in the IOURING_ENTER_GETEVENTS flag which causes the
     * io_uring_enter() call to wait until min_complete events (the 3rd param)
     * complete.
     * */
    int ret =  io_uring_enter(s->ring_fd, 1,1,
            IORING_ENTER_GETEVENTS);
    if(ret < 0) {
        perror("io_uring_enter");
        return 1;
    }

    return 0;
}

int main(int argc, char *argv[]) {
    struct submitter *s;

    if (argc < 2) {
        fprintf(stderr, "Usage: %s <filename>\n", argv[0]);
        return 1;
    }

    s = malloc(sizeof(*s));
    if (!s) {
        perror("malloc");
        return 1;
    }
    memset(s, 0, sizeof(*s));

    if(app_setup_uring(s)) {
        fprintf(stderr, "Unable to setup uring!\n");
        return 1;
    }

    for (int i = 1; i < argc; i++) {
        if(submit_to_sq(argv[i], s)) {
            fprintf(stderr, "Error reading file\n");
            return 1;
        }
        read_from_cq(s);
    }

    return 0;
}
```

## 동작방식 분석

![io_uring_setup](/assets/img/iouring_setup.png)
io_uring에서 파일 디스크립터는 먼저 전용 시스템 호출(io_uring_setup)에 의해 생성된다. io_uring_setup시에 커널에서는 fd를 리턴하기 위한 여러가지 설정을 해 주는데 sq_entries, cq_entries등에 대한 세팅과 io_uring_ctx구조체의 초기화를 통하여 구조체 초기화 등을 한다. io_uring_setup에서 변수로 보내주는 io_uring_params안에 있는

```c
io_uring_params {
	sq_entries
	cq_entries
	flags
	...
	sq_off
	cq_off
}
```

여러가지 변수들도 이때 같이 채워진다. 이후 fd가 리턴되고 나면 해당 fd로 mmap() 시스템 호출을 발행하여 제출 큐(SQ)와 완료 큐(CQ)가 사용자 공간 메모리에 매핑/공유한다.
이것은 양쪽(Kernel/Userspace)에서 링 버퍼로 사용된다. read/write/send/recv와 같은 각 시스템 호출에 대한 항목은 SQE(Submission Queue Entry)를 공유 메모리에 작성하여 등록되며 io_uring_enter()를 호출하여 실행을 시작한다.

## 비동기 실행

중요한 부분은 비동기 실행의 구현이므로 그것에 중점을 둔다. 먼저 설명하자면 io_uring이 항상 비동기적으로 실행되는 것은 아니지만 필요에 따라 비동기적으로 실행된다. 먼저 아래의 코드를 참고하시기 바랍니다. (안드로이드 커널 5.10을 기준으로 설명)
```c
#define _GNU_SOURCE
#include <sched.h>
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <signal.h>
#include <sys/syscall.h>
#include <sys/fcntl.h>
#include <err.h>
#include <unistd.h>
#include <sys/mman.h>
#include <linux/io_uring.h>
#define SYSCHK(x) ({          \
  typeof(x) __res = (x);      \
  if (__res == (typeof(x))-1) \
    err(1, "SYSCHK(" #x ")"); \
  __res;                      \
})
static int uring_fd;
struct iovec *io;
#define SIZE 32
char _buf[SIZE];
int main(void) {
  // initialize uring
  struct io_uring_params params = { };
  uring_fd = SYSCHK(syscall(__NR_io_uring_setup, /*entries=*/10, &params));
  unsigned char *sq_ring = SYSCHK(mmap(NULL, 0x1000, PROT_READ|PROT_WRITE,
                                       MAP_SHARED, uring_fd,
                                       IORING_OFF_SQ_RING));
  unsigned char *cq_ring = SYSCHK(mmap(NULL, 0x1000, PROT_READ|PROT_WRITE,
                                       MAP_SHARED, uring_fd,
                                       IORING_OFF_CQ_RING));
  struct io_uring_sqe *sqes = SYSCHK(mmap(NULL, 0x1000, PROT_READ|PROT_WRITE,
                                          MAP_SHARED, uring_fd,
                                          IORING_OFF_SQES));
io = malloc(sizeof(struct iovec)*1);
  io[0].iov_base = _buf;
  io[0].iov_len = SIZE;
struct timespec ts = { .tv_sec = 1 };
  sqes[0] = (struct io_uring_sqe) {
    .opcode = IORING_OP_TIMEOUT,
    //.flags = IOSQE_IO_HARDLINK,
    .len = 1,
    .addr = (unsigned long)&ts
  };
  sqes[1] = (struct io_uring_sqe) {
    .opcode = IORING_OP_READV,
    .addr = io,
    .flags = 0,
    .len = 1,
    .off = 0,
    .fd = SYSCHK(open("/etc/passwd", O_RDONLY))
  };
  ((int*)(sq_ring + params.sq_off.array))[0] = 0;
  ((int*)(sq_ring + params.sq_off.array))[1] = 1;
  (*(int*)(sq_ring + params.sq_off.tail)) += 2;
int submitted = SYSCHK(syscall(__NR_io_uring_enter, uring_fd,
                                 /*to_submit=*/2, /*min_complete=*/0,
                                 /*flags=*/0, /*sig=*/NULL, /*sigsz=*/0));
  while(1){
    usleep(100000);
    if(*_buf){
      puts("READV executed.");
      break;
    }
    puts("Waiting.");
  }
}
```

이 코드에서는 IORING_OP_TIMEOUT 및 IORING_OP_READV 작업에 필요한 설정을 수행한 후 실행을 시작한 다음 0.1초마다 readv()가 완료되었는지 확인한다. 링 버퍼의 순서로 실행되는 것을 감안할 때 readv()는 1초 후에 완료될 것으로 보인다. 그런데 실제로 실행해 보니 바로 실행되었다.

```sh
$ ./sample
READV executed.
```

즉, readv()의 실행이 즉시 완료되었는데 앞서 말했듯이 필요에 따라 비동기적으로 실행되지만 이 경우 readv() 실행을 즉시 완료할 수 있기 때문에 즉시 실행한다. 그래서 IORING_OP_TIMEOUT보다 IORING_OP_READ 작업이 먼저 완료되었다. 테스트로 다음 systemtap[¹] 스크립트를 사용하여 readv()가 동기적으로(= 시스템 호출 핸들러에서) 실행되는지 확인한다.

```sh
#!/usr/bin/stap
probe kernel.function("io_read@/build/linux-b4NE0x/linux-5.8.0/fs/io_uring.c:2710"){
  printf("%s\n",task_execname(task_current()))
}

$ sudo stap -g ./sample.stp
sample
```

위의 systemtap 스크립트가 실행되는 동안 프로그램을 실행했을 때의 출력이다. 비동기식이라면 실행 태스크가 어떤 워커에 등록되어 있다고 상상하기 쉽지만 여기서는 동기식으로 실행되기 때문에 시스템 호출을 호출한 실행 파일의 이름이 출력된다.
![io_uring_async1](/assets/img/iouring_async1.png)


IORING_OP_TIMEOUT은 비동기 실행이 필요하다고 판단되어 커널 스레드로 전달되었다. 이에 대한 몇 가지 기준이 있으며 충족하면 비동기 실행을 위해 대기열에 추가된다.
1. force async flag가 설정될 때

```c
static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
			 struct io_comp_state *cs)
{
...
	} else if (req->flags & REQ_F_FORCE_ASYNC) {
		if (!req->async_data) {
			ret = io_req_defer_prep(req, sqe);
			if (unlikely(ret))
				goto fail_req;
		}
		io_queue_async_work(req);
...
}
```

2. 각 연산에 ​​대해 로직에 의한 결정. (예: readv()를 호출할 때 IOCB_NOWAIT 플래그를 추가하고 실행이 중지될 것으로 예상되면 EAGAIN을 반환)
해당 코드는 아직 분석이 완벽하지 않아 조금 더 분석이 필요함.
```c
static int io_read(struct io_kiocb *req, bool force_nonblock,
		   struct io_comp_state *cs)
{
...
	ret = io_iter_do_read(req, iter);
	else if (ret == -EAGAIN) {
		...
		goto copy_iov;
	}
copy_iov:
	ret2 = io_setup_async_rw(req, iovec, inline_vecs, iter, true);
	rw = req->async_data;
	/* it's copied and will be cleaned with ->io */
	iovec = NULL;
	/* now use our persistent iterator, if we aren't already */
	iter = &rw->iter;
retry:
	rw->bytes_done += ret;
	/* if we can retry, do so with the callbacks armed */
	if (!io_rw_should_retry(req)) {
		kiocb->ki_flags &= ~IOCB_WAITQ;
		return -EAGAIN;
	}
}

/*
 * This controls whether a given IO request should be armed for async page
 * based retry. If we return false here, the request is handed to the async
 * worker threads for retry. If we're doing buffered reads on a regular file,
 * we prepare a private wait_page_queue entry and retry the operation. This
 * will either succeed because the page is now uptodate and unlocked, or it
 * will register a callback when the page is unlocked at IO completion. Through
 * that callback, io_uring uses task_work to setup a retry of the operation.
 * That retry will attempt the buffered read again. The retry will generally
 * succeed, or in rare cases where it fails, we then fall back to using the
 * async worker threads for a blocking retry.
 */
static bool io_rw_should_retry(struct io_kiocb *req)
{
...
}


static void __io_queue_sqe(struct io_kiocb *req, struct io_comp_state *cs)
{
...
again:
	ret = io_issue_sqe(req, true, cs);

	/*
	 * We async punt it if the file wasn't marked NOWAIT, or if the file
	 * doesn't support non-blocking read/write attempts
	 */
	if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {
		if (!io_arm_poll_handler(req)) {
			/*
			 * Queued up for async execution, worker will release
			 * submit reference when the iocb is actually submitted.
			 */
			io_queue_async_work(req);
		}
```

3. IOSQE_IO_LINK|IOSQE_IO_HARDLINK 플래그를 사용하고(실행 순서 지정) 실행 순서가 빠른 연산이 비동기 실행이 필요한 것으로 판단되는 경우.
(아래 코드에 설명된 대로 링크로 연결하여 순서대로 실행하고 중간에 조건 2가 충족되면 전체 링크가 비동기 실행 큐에 대기열에 추가됨.)

```c
static int io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
			 struct io_kiocb **link, struct io_comp_state *cs)
{
...	} else {
		if (unlikely(ctx->drain_next)) {
			req->flags |= REQ_F_IO_DRAIN;
			ctx->drain_next = 0;
		}
		if (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {
			req->flags |= REQ_F_LINK_HEAD;
			INIT_LIST_HEAD(&req->link_list);

			ret = io_req_defer_prep(req, sqe);
			if (unlikely(ret))
				req->flags |= REQ_F_FAIL_LINK;
			*link = req;
		} else {
			io_queue_sqe(req, sqe, cs);
		}
	}

	return 0;
}

```

엄밀히 말하면 IORING_OP_TIMEOUT은 조금 특이해서 2와 같이 EAGAIN을 반환하지 않고 단순히 타이머가 돌 뿐이다. 하지만 아래와 같이 비동기 실행이 필요한 연산(IORING_OP_TIMEOUT)을 다른 연산과 연결하면 1초를 기다린 후 이전 IORING_OP_READV가 확실히 실행되는 것을 확인할 수 있다.

```sh
$ ./sample
Waiting.
Waiting.
Waiting.
Waiting.
Waiting.
Waiting.
Waiting.
Waiting.
Waiting.
READV executed.

$ sudo stap -g ./sample.stp
io_wqe_worker-0
```

'worker thread'는 다음 코드에 의해 생성된 다음 Queue에서 비동기 실행 작업을 대기열에서 빼고 실행한다.
```c
static bool create_io_worker(struct io_wq *wq, struct io_wqe *wqe, int index)
{
 ......
worker->task = kthread_create_on_node(io_wqe_worker, worker, wqe->node,
    "io_wqe_worker-%d/%d", index, wqe->node);
 ......
}
```

앞에서 설명한 것처럼 IORING_OP_TIMEOUT은 아래 그림과 약간 다르게 동작하지만 단순함을 위해 그렇게 설명한다. 엄밀히 말하면 io_timeout()이 호출되면 핸들러에서 io_timeout_fn()을 설정하고 타이머를 시작한다. 타이머가 설정한 시간이 지나면 io_timeout_fn()이 호출되어 비동기 실행 큐의 링크에 연결된 작업을 실행한다. 즉, IORING_OP_TIMEOUT 자체는 비동기 실행 대기열에 포함되지 않는다. 
![io_uring_async2](/assets/img/iouring_async2.png)


## Precautions when offloading I/O operations to the Kernel

비동기 처리는 Kernel worker thread 의해 수행된다. 그러나 여기에는 주의할 점이 있는데 worker thread는 커널 권한으로 실행되기 때문에 실행 컨텍스트는 io_uring 관련 시스템 호출을 호출하는 스레드와 다르다. 여기서 '실행 컨텍스트'는 프로세스와 관련된 task_struct 구조 및 이와 관련된 다양한 정보를 의미한다. 예를 들어, mm(프로세스의 가상 메모리 공간 관리), cred(UID/GID/Capability 보유),files_struct(파일 디스크립터에 대한 테이블 보유. files_struct 구조에는 파일 구조 배열이 있으며 파일 디스크립터는 해당 인덱스.) 시스템 호출을 호출하는 worker 스레드에서 이러한 구조를 참조하지 않으면 잘못된 가상 메모리 또는 파일 디스크립터 테이블을 참조하거나 커널 스레드 권한(≒ 루트)으로 I/O 작업을 실행할 수 있으며 이에 대한 취약점도 발견 되었다.

[*] : 이것은 실제 취약점이었고 당시에는 cred를 전환하는 것을 잊어 버렸고 루트 권한으로 작업을 실행할 수 있었다. open()에 해당하는 연산은 그 당시 구현되지 않았지만 sendmsg의 SCM_CREDENTIALS 옵션에서 권한을 보낸 사람의 권한을 알려주는 것이 가능헀음. https://www.exploit-db.com/exploits/47779

따라서 io_uring에서 이러한 참조는 작업자에게 전달되어 작업자가 실행 전에 자체 컨텍스트를 전환하여 실행 컨텍스트를 공유한다. 예를 들어 mm 및 cred에 대한 참조가 다음 코드에서 전달된다.
```c
static void io_init_identity(struct io_identity *id)
{
	id->files = current->files;
	id->mm = current->mm;
#ifdef CONFIG_BLK_CGROUP
	rcu_read_lock();
	id->blkcg_css = blkcg_css();
	rcu_read_unlock();
#endif
	id->creds = current_cred();
	id->nsproxy = current->nsproxy;
	id->fs = current->fs;
	id->fsize = rlimit(RLIMIT_FSIZE);
#ifdef CONFIG_AUDIT
	id->loginuid = current->loginuid;
	id->sessionid = current->sessionid;
#endif
	refcount_set(&id->count, 1);

}
```

그런 다음 실행 전에 작업자의 현재 내용으로 대체된다.

```c
static void io_impersonate_work(struct io_worker *worker,
				struct io_wq_work *work)
{
	if ((work->flags & IO_WQ_WORK_FS) && current->fs != work->identity->fs)
		current->fs = work->identity->fs;
	if ((work->flags & IO_WQ_WORK_MM) && work->identity->mm != worker->mm)
		io_wq_switch_mm(worker, work);
	if ((work->flags & IO_WQ_WORK_CREDS) &&
	    worker->cur_creds != work->identity->creds)
		io_wq_switch_creds(worker, work);
	if (work->flags & IO_WQ_WORK_FSIZE)
		current->signal->rlim[RLIMIT_FSIZE].rlim_cur = work->identity->fsize;
	else if (current->signal->rlim[RLIMIT_FSIZE].rlim_cur != RLIM_INFINITY)
		current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
	io_wq_switch_blkcg(worker, work);
	...
}
```
![io_uring_offloading](/assets/img/iouring_offloading.png)

# 취약점 설명

## Reference counter in files_struct structure when sharing with the worker
아래  코드(이전에 게시)에서 작업자가 시스템 호출을 실행하는 스레드의 files_struct 구조에 대한 참조를 참조 카운터를 증가시키지 않고 작업자가 나중에 참조할 구조로 전달하는 것을 볼 수 있다.


```c
static void io_init_identity(struct io_identity *id)
{
	id->files = current->files;
...
}
```


앞에서 간단히 설명했듯이 비동기 실행을 위해 대기열에 작업을 대기열에 넣을 때 file descriptor로부터 가져온 file struct가 async queue에 저장된다.
![io_uring_file1](/assets/img/iouring_file1.png)

따라서 worker thread는 file descriptor를 다시 검색할 필요가 없으며 files_struct 구조를 참조할 필요도 없다. 그렇다면 files_struct 구조체의 참조 카운터가 증가하지 않는(사용하지 않기 때문에) 문제는 없는 것 같다.
그러나 이 가정은 사실이 아닌데, open/close/accept 와 같은 파일 디스크립터 테이블에 영향을 미치는 시스템 호출이 이제 io_uring을 통해 사용 가능하기 때문이다. 분명히 이러한 시스템 호출은 파일 디스크립터 테이블에 영향을 미치므로 악용에 사용될 수 있는 것처럼 보이지만,

- 단순히 open/close/accept 등을 호출하더라도 files_struct 구조가 사용 가능하면 아무 일도 일어나지 않습니다. 물론 여러 쓰레드에서 같은 파일을 핸들링하는 시스템콜에 대한 대비책이 있기 때문에 간단히 worker thread와 호출 쓰레드간의 race condition이 발생하진 않는다.
- 참조 카운터를 0으로 설정하여 files_struct를 해제하면 새 프로세스에서 해당 프로세스의 files_struct로 재사용할 수 있습니다. 작업자는 재사용될 때 새 프로세스의 files_struct에 대한 참조를 얻습니다.

다음으로 동일한 파일을 여러 스레드에서 처리할 때의 file struct의 reference counter를 중심으로 메커니즘을 설명한다. 결론은 실제로 남용될 수 있다는 것이다.

## Mechanism of reference counter in open/close system call

파일 구조의 참조 카운터가 어떻게 작동하는지 이해하려면 먼저 open/close가 실제로 무엇을 하는지에 대해 설명한다. 물론 실제 열려는 파일에 따라 동작이 달라지긴 하지만 공통점은 다음과 같다.

open:
1. 파일 구조를 만들고 참조 카운터를 1로 설정합니다.
2. 파일 설명자 테이블에 등록

```c
static struct file *__alloc_file(int flags, const struct cred *cred)
{
 struct file *f;
 int error;
f = kmem_cache_zalloc(filp_cachep, GFP_KERNEL);
 ......
 atomic_long_set(&f->f_count, 1);
 ......
 return f;
}
```

파일 디스크립터 테이블(fd_install)에 등록.
```c
static long do_sys_openat2(int dfd, const char __user *filename,
      struct open_how *how)
{
 ......
 fd = get_unused_fd_flags(how->flags);
 if (fd >= 0) {
  struct file *f = do_filp_open(dfd, tmp, &op);
  if (IS_ERR(f)) {
   put_unused_fd(fd);
   fd = PTR_ERR(f);
  } else {
   fsnotify_open(f);
   fd_install(fd, f);
  }
 }
 putname(tmp);
 return fd;
}
```

close:
1. 파일 설명자 테이블에서 삭제
2. 파일 구조의 참조 카운터를 감소시킴.(fput)

여기서 중요한 것은 fget()/fput() 함수이다(fget()는 open에서 사용되지는 않지만). 이것들은 파일 구조의 참조 카운터를 증가/감소시키며 참조가 0에 도달하면 fput()은 파일 구조의 메모리를 해제한다. 이 메커니즘 덕분에 fget()으로 파일 구조를 가져오면 참조 카운터는 fput() 이전에 닫혀 있어도 0이 되지 않는다.(카운터는 열릴 때 1, fget()을 호출한 후에 2이어야 하며, 이때 닫혀 있어도 1입니다.). 따라서 사용 중 닫아도 문제가 없다는 의미이다.

예를 들어 mmap을 사용하여 파일을 메모리에 매핑할 때 닫기 후에도 munmap을 호출하기 전에 메모리가 해제되면 문제가 된다. 따라서 fget()은 메모리가 해제되는 것을 방지하기 위해 mmap에서 사용됩니다.

```c
unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
         unsigned long prot, unsigned long flags,
         unsigned long fd, unsigned long pgoff)
{
 struct file *file = NULL;
 unsigned long retval;
if (!(flags & MAP_ANONYMOUS)) {
  audit_mmap_fd(fd, flags);
  file = fget(fd);
 ......
}
```

## fdget


